{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae96354e",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation\n",
    "\n",
    "This is the takehome notebook for the NLP engineer position at Contenda. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b7e3971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models import Phrases, CoherenceModel\n",
    "from gensim import corpora, models, parsing\n",
    "\n",
    "import nltk\n",
    "\n",
    "import os\n",
    "from typing import List, Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "57b68a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/samanthawilcoxson/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/samanthawilcoxson/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/samanthawilcoxson/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/samanthawilcoxson/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/samanthawilcoxson/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a749c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5b0947f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Functions\n",
    "\n",
    "custom_stopwords = set(['gon', 'na', 'yep'])\n",
    "stopwords = set(parsing.preprocessing.STOPWORDS).union(custom_stopwords)\n",
    "\n",
    "def remove_stopwords(list_tokens: List[Text]):\n",
    "    return [w for w in list_tokens if w not in stopwords]\n",
    "\n",
    "def preprocess(list_text: List[Text]) -> List:\n",
    "    df = pd.DataFrame(list_text)\n",
    "    df.columns = [\"documents\"]\n",
    "    df['sentences'] = df.documents.map(sent_tokenize)\n",
    "    df['tokens_sentences'] = df['sentences'].map(lambda sentences: [word_tokenize(sentence) for sentence in sentences])\n",
    "    df['POS_tokens'] = df['tokens_sentences'].map(lambda tokens_sentences: [pos_tag(tokens) for tokens in tokens_sentences])\n",
    "    df['tokens_sentences_lemmatized'] = df['POS_tokens'].map(\n",
    "        lambda list_tokens_POS: [\n",
    "            [\n",
    "                lemmatizer.lemmatize(el[0], get_wordnet_pos(el[1])) \n",
    "                if get_wordnet_pos(el[1]) != '' else el[0] for el in tokens_POS\n",
    "            ] \n",
    "            for tokens_POS in list_tokens_POS\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    df['tokens'] = df['tokens_sentences_lemmatized'].map(lambda sentences: list(chain.from_iterable(sentences)))\n",
    "    df['tokens'] = df['tokens'].map(lambda tokens: [token.lower() for token in tokens if token.isalpha()])\n",
    "    \n",
    "    # remove stopwords\n",
    "    df['tokens'] = df['tokens'].map(lambda tokens: remove_stopwords(tokens))\n",
    "\n",
    "    # obtain unigrams, bigrams, trigrams\n",
    "    tokens = df['tokens'].tolist()\n",
    "    bigram_model = Phrases(tokens)\n",
    "    trigram_model = Phrases(bigram_model[tokens], min_count=1)\n",
    "    tokens = list(trigram_model[bigram_model[tokens]])\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "4146dd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lda_model(list_text, num_topics):\n",
    "    # preprocess\n",
    "    tokens = preprocess(list_text)\n",
    "\n",
    "    # build dictionary\n",
    "    dictionary_LDA = corpora.Dictionary(tokens)\n",
    "    dictionary_LDA.filter_extremes(no_below=0)\n",
    "    corpus = [dictionary_LDA.doc2bow(tok) for tok in tokens]\n",
    "    \n",
    "    # build model\n",
    "    lda_model = models.LdaModel(corpus, num_topics=num_topics, \\\n",
    "                                      id2word=dictionary_LDA, \\\n",
    "                                      passes=4, alpha=[0.01]*num_topics, \\\n",
    "                                      eta=[0.01]*len(dictionary_LDA.keys()),\n",
    "                                      random_state=10)\n",
    "    \n",
    "    return lda_model, dictionary_LDA\n",
    "\n",
    "def eval_model(model, test_corpus):\n",
    "    results = {\n",
    "        'perplexity': model.log_perplexity(test_corpus),\n",
    "        'coherence':  CoherenceModel(model=model, corpus=test_corpus, coherence='u_mass').get_coherence()\n",
    "    }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "543c43fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "train = [open(f\"./training_transcriptions/{fname}\").read() for fname in os.listdir(\"./training_transcriptions\")]\n",
    "test = [open(f\"./testing_transcriptions/{fname}\").read() for fname in os.listdir(\"./testing_transcriptions\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "ffd98f32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.003*\"corgi\" + 0.003*\"cli\" + 0.003*\"laughter\" + 0.002*\"chat\" + 0.002*\"link\" + 0.002*\"application\" + 0.002*\"rust\" + 0.002*\"github\" + 0.002*\"function\" + 0.002*\"javascript\"'),\n",
       " (1,\n",
       "  '0.006*\"design\" + 0.005*\"button\" + 0.005*\"database\" + 0.004*\"mongodb\" + 0.004*\"target\" + 0.004*\"engineer\" + 0.003*\"mmhmm\" + 0.003*\"image\" + 0.003*\"color\" + 0.003*\"search\"'),\n",
       " (2,\n",
       "  '0.004*\"snake\" + 0.003*\"image\" + 0.003*\"game\" + 0.003*\"typescript\" + 0.003*\"site\" + 0.003*\"laughter\" + 0.002*\"react\" + 0.002*\"twilioquest\" + 0.002*\"chat\" + 0.002*\"request\"')]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, dictionary_LDA = build_lda_model(train, num_topics=3)\n",
    "model.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "8db85fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.3078507), (1, 0.23757707), (2, 0.45457223)]\n"
     ]
    }
   ],
   "source": [
    "# preprocess data in model input format\n",
    "train_corpus = [dictionary_LDA.doc2bow(doc) for doc in preprocess(train)]  # we'll use this for evaluation later\n",
    "test_corpus = [dictionary_LDA.doc2bow(doc) for doc in preprocess(test)]\n",
    "\n",
    "preds = [model[doc] for doc in test_corpus]\n",
    "print(preds[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "47af3737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>documents</th>\n",
       "      <th>predictions</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Why we build. We build because we see potentia...</td>\n",
       "      <td>[(0, 0.3078507), (1, 0.23757707), (2, 0.454572...</td>\n",
       "      <td>0.307851</td>\n",
       "      <td>0.237577</td>\n",
       "      <td>0.454572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hello, everyone, and welcome to another episod...</td>\n",
       "      <td>[(0, 0.5027197), (1, 0.033822425), (2, 0.46345...</td>\n",
       "      <td>0.502720</td>\n",
       "      <td>0.033822</td>\n",
       "      <td>0.463458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tamao: Leandro here has fantastic story, you’r...</td>\n",
       "      <td>[(0, 0.39150605), (1, 0.19409886), (2, 0.41439...</td>\n",
       "      <td>0.391506</td>\n",
       "      <td>0.194099</td>\n",
       "      <td>0.414395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hello, everyone, and welcome to another episod...</td>\n",
       "      <td>[(0, 0.39435107), (1, 0.073617354), (2, 0.5320...</td>\n",
       "      <td>0.394351</td>\n",
       "      <td>0.073617</td>\n",
       "      <td>0.532032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>So who remembers GeoCities? Thank you. Yeah. O...</td>\n",
       "      <td>[(0, 0.30412647), (1, 0.2323362), (2, 0.463537...</td>\n",
       "      <td>0.304126</td>\n",
       "      <td>0.232336</td>\n",
       "      <td>0.463537</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           documents  \\\n",
       "0  Why we build. We build because we see potentia...   \n",
       "1  Hello, everyone, and welcome to another episod...   \n",
       "2  Tamao: Leandro here has fantastic story, you’r...   \n",
       "3  Hello, everyone, and welcome to another episod...   \n",
       "4  So who remembers GeoCities? Thank you. Yeah. O...   \n",
       "\n",
       "                                         predictions         0         1  \\\n",
       "0  [(0, 0.3078507), (1, 0.23757707), (2, 0.454572...  0.307851  0.237577   \n",
       "1  [(0, 0.5027197), (1, 0.033822425), (2, 0.46345...  0.502720  0.033822   \n",
       "2  [(0, 0.39150605), (1, 0.19409886), (2, 0.41439...  0.391506  0.194099   \n",
       "3  [(0, 0.39435107), (1, 0.073617354), (2, 0.5320...  0.394351  0.073617   \n",
       "4  [(0, 0.30412647), (1, 0.2323362), (2, 0.463537...  0.304126  0.232336   \n",
       "\n",
       "          2  \n",
       "0  0.454572  \n",
       "1  0.463458  \n",
       "2  0.414395  \n",
       "3  0.532032  \n",
       "4  0.463537  "
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get preds in df format\n",
    "preds_dicts = [dict(x) for x in preds]\n",
    "preds_df = pd.DataFrame(preds_dicts)\n",
    "\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'documents': test,\n",
    "    'predictions': preds\n",
    "})\n",
    "\n",
    "for i in range(len(model.get_topics())):\n",
    "    df[i] = preds_df[i] if i in preds_df else np.nan\n",
    "\n",
    "df = df.fillna(0)\n",
    "\n",
    "df.to_csv(\"./results.csv\", index=False)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "82e3bd71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'perplexity': -9.737015962832338, 'coherence': -1.6580351979459576}\n",
      "{'perplexity': -11.807010213357163, 'coherence': -3.3234591095030943}\n"
     ]
    }
   ],
   "source": [
    "results_on_train = eval_model(model, [dictionary_LDA.doc2bow(tok) for tok in preprocess(train)])\n",
    "results_on_test = eval_model(model, test_corpus)\n",
    "print(results_on_train)\n",
    "print(results_on_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf1e8a8",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "To determine the number of topics to extract, I tried running the model on `num_topics={2,3,5,10}` and eyeballing the topic keywords (`model.print_topics()`) to see if they made sense. 3 seemed to be the most interpretable. 2 just grouped the documents into devrel vs. Learn With Jason, which didn't provide much new information on the test documents. The topics in 5 were too similar to each other, and 10 was too many to identify clear stratification.\n",
    "\n",
    "Looking strictly at the keywords, our 3 topics can be described by the following categories:\n",
    "\n",
    "0. general tech terminology (seems to come from the tutorials)\n",
    "1. application development and design\n",
    "2. games (and application development)\n",
    "\n",
    "This seems to represent our data with rough accuracy; if we take a probability of 0.1 as the classification threshold, the Facebook talk (`test[0]`) covers all 3 topics, the Brandon Roberts episode of Learn With Jason (`test[1]`) covers topics 0 and 2, and the devrel talk starting with Tamao (`test[2]`) covers all 3 as well. Except `test[2]` doesn't really talk about gaming! It seems like 3 isn't really the magic number of topics, here, but within the allotted time, it provides the most \"explainable\" categories.\n",
    "\n",
    "I used log perplexity and coherence as evaluation metrics as outlined in this tutorial, which seems consistent with the industry standard:\n",
    "https://www.tutorialspoint.com/gensim/gensim_using_lda_topic_model.htm\n",
    "\n",
    "Here's a good breakdown of why perplexity is useful for this task:\n",
    "https://cfss.uchicago.edu/notes/topic-modeling/#:~:text=Perplexity%20is%20a%20statistical%20measure,of%20words%20in%20your%20documents\n",
    "\n",
    "These are the sources I used to interpret the coherence score, since that tutorial doesn't provide a great explanation:\n",
    "https://www.baeldung.com/cs/topic-modeling-coherence-score (good breakdown of types of coherence, but their interpretation of what a greater number means is incorrect according to other sources)\n",
    "https://aclanthology.org/D12-1087.pdf (nice visual representation of UMass vs UCI coherence)\n",
    "https://ciir-publications.cs.umass.edu/getpdf.php?id=956 (see 4.2 - Topic Coherence)\n",
    "\n",
    "According to these metrics, a score closer to 0 means that the document is most similar to the training set, or documents that the model has seen before. Our perplexity on our testing set is `-11.807010213357163`, and coherence on our testing set is `-3.3234591095030943`. While these numbers don't mean much on their own, they seem similar enough to the training data (`perplexity=-9.737015962832338, coherence=-1.6580351979459576`). This implies the model is relatively confident in its ability to predict topics from the test set.\n",
    "\n",
    "If I had more time, there are 2 clear improvements that would make the results more interpretable: fine tuning num_topics with a grid search or similar, allowing us to compare the coherence and perplexity scores to find the best n to stratify with, and better stopword pruning to filter out noise.\n",
    "\n",
    "I learned a lot about LDA and evaluation as I went, and ran out of time before implementing some (now obvious) changes that would make the results more useful in a production environment. I hope that this writeup provides some insight into how I handle unfamiliar problems, interpret results, and explain my interpretations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e92ca9c",
   "metadata": {},
   "source": [
    "How did you allocate your time?\n",
    "* ~20 mins looking at data and familiarizing self with task\n",
    "* ~20 mins researching LDA architecture and how it works\n",
    "* ~20 mins researching LDA evaluation\n",
    "* ~90 mins implementing and experimenting\n",
    "* ~30 mins writeup/reflections\n",
    "\n",
    "\n",
    "What are the tradeoffs between training a model on transcripts as a corpus versus written articles?\n",
    "* Transcripts are noisier, as they are a representation of speech. Transcripts often contain more filler words, self-corrections, backtracking, and other quirks that come with spontaneous language production. As a result, parsing may be more of a challenge, but a model trained on transcripts will do a lot better in the spoken language domain than one trained on newswire data.\n",
    "\n",
    "\n",
    "What is something you'd like to try if you had another 8 hours? 3 days? 1 week?\n",
    "* in order of least to most time consuming:\n",
    "    - output top 3 topics for each doc to a separate column for easy viewing\n",
    "    - add to stopword list (names, adjectives, filler words like 'mhmm', etc)\n",
    "    - experiment with num_topics\n",
    "    - tune other hyperparameters\n",
    "    - try a spacy model for parsing: https://spacy.io/models/en\n",
    "    - try a BERT embedding-based topic model: https://github.com/MaartenGr/KeyBERT\n",
    "    - manually annotate a subset of train/test data for more interpetable evaluation\n",
    "\n",
    "What are the tradeoffs between using LDA for topic modeling vs other methods?\n",
    "* LDA doesn't take word context into account since it uses a bag-of-words method. That makes it a lot simpler and faster to run than a context based model, but your topics are limited. Evaluation is also difficult because LDA is unsupervised. Supervised methods are easier to evaluate and very interpretable, but require time and resources to annotate, topics must be selected beforehand, and might be very narrow or very broad. Any keyword-based topic model is subsequently going to be limited by the words that occur in the document, but might generalize better.\n",
    "\n",
    "Machine transcriptions have more mistakes than human transcriptions. If we only had access to a large amount of machine transcriptions, what are some strategies we could try to still have decent topic modeling?\n",
    "- run a parser over sentences in each document to make sure sentences are grammatical\n",
    "- manually check a subsection and calculate WER\n",
    "- identify points of confusion using a masked/warped language model: https://www.amazon.science/blog/using-warped-language-models-to-correct-speech-recognition-errors\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6a9fcaafee66f84e2b28f61edd863308082ed633d08a8f6acaf3e5a53c17c457"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
